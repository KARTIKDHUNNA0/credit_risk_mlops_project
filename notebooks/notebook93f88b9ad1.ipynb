{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T15:15:40.086899Z","iopub.execute_input":"2025-11-21T15:15:40.087714Z","iopub.status.idle":"2025-11-21T15:15:40.094522Z","shell.execute_reply.started":"2025-11-21T15:15:40.087681Z","shell.execute_reply":"2025-11-21T15:15:40.093827Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/modelll/submission_xgboost.csv\n/kaggle/input/modelll/train_FINAL.pkl\n/kaggle/input/modelll/submission_lightgbm.csv\n/kaggle/input/modelll/submission_catboost.csv\n/kaggle/input/modelll/test_FINAL.pkl\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================\n# 0. Install libraries (Kaggle GPU)\n# ================================\n!pip install optuna catboost lightgbm xgboost --quiet\n\nimport pandas as pd\nimport numpy as np\nimport optuna\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"GPU test:\")\n!nvidia-smi\n\n# ================================\n# 1. Load final data\n# ================================\ntrain = joblib.load(\"/kaggle/input/modelll/train_FINAL.pkl\")\ntest  = joblib.load(\"/kaggle/input/modelll/test_FINAL.pkl\")\n\ny = train[\"TARGET\"]\nX = train.drop(columns=[\"TARGET\"])\n\ntest_ids = test[\"SK_ID_CURR\"]\ntest_X = test.drop(columns=[\"SK_ID_CURR\"])\n\n# Ensure clean column names\nX.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\ntest_X.columns = test_X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n\n\n# ================================\n# 2. KFold setup\n# ================================\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n# ================================\n# 3. Optuna search space + trial objective\n# ================================\ndef objective(trial):\n\n    model_type = trial.suggest_categorical(\"model_type\", [\"lightgbm\", \"xgboost\", \"catboost\"])\n\n    # ------------------\n    # LightGBM\n    # ------------------\n    if model_type == \"lightgbm\":\n        params = {\n            \"objective\": \"binary\",\n            \"metric\": \"auc\",\n            \"learning_rate\": trial.suggest_float(\"lr\", 0.01, 0.3),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 200),\n            \"verbosity\": -1\n        }\n\n        oof_preds = np.zeros(len(X))\n\n        for train_idx, valid_idx in kf.split(X):\n            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n            dtrain = lgb.Dataset(X_train, y_train)\n            dvalid = lgb.Dataset(X_valid, y_valid)\n\n            model = lgb.train(\n                params,\n                dtrain,\n                valid_sets=[dvalid],\n                num_boost_round=2000,\n                callbacks=[lgb.early_stopping(100, verbose=False)]\n            )\n\n            preds = model.predict(X_valid, num_iteration=model.best_iteration)\n            oof_preds[valid_idx] = preds\n\n        return roc_auc_score(y, oof_preds)\n\n    # ------------------\n    # XGBoost (GPU)\n    # ------------------\n    if model_type == \"xgboost\":\n        params = {\n            \"objective\": \"binary:logistic\",\n            \"eval_metric\": \"auc\",\n            \"tree_method\": \"gpu_hist\",  # GPU acceleration\n            \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n        }\n\n        oof_preds = np.zeros(len(X))\n\n        for train_idx, valid_idx in kf.split(X):\n            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n            dtrain = xgb.DMatrix(X_train, y_train)\n            dvalid = xgb.DMatrix(X_valid, y_valid)\n\n            model = xgb.train(\n                params,\n                dtrain,\n                num_boost_round=2000,\n                evals=[(dvalid, \"valid\")],\n                early_stopping_rounds=100,\n                verbose_eval=False\n            )\n\n            preds = model.predict(xgb.DMatrix(X_valid))\n            oof_preds[valid_idx] = preds\n\n        return roc_auc_score(y, oof_preds)\n\n    # ------------------\n    # CatBoost (GPU)\n    # ------------------\n    if model_type == \"catboost\":\n        params = {\n            \"iterations\": 2000,\n            \"learning_rate\": trial.suggest_float(\"lr\", 0.01, 0.3),\n            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n            \"loss_function\": \"Logloss\",\n            \"eval_metric\": \"AUC\",\n            \"task_type\": \"GPU\",\n            \"verbose\": False\n        }\n\n        oof_preds = np.zeros(len(X))\n\n        for train_idx, valid_idx in kf.split(X):\n            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n            model = CatBoostClassifier(**params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=100)\n\n            preds = model.predict_proba(X_valid)[:, 1]\n            oof_preds[valid_idx] = preds\n\n        return roc_auc_score(y, oof_preds)\n\n\n# ================================\n# 4. Run Optuna with 10 trials\n# ================================\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=10)\n\nprint(\"Best params:\", study.best_params)\nprint(\"Best AUC:\", study.best_value)\n\n# ================================\n# 5. Train final model on full data using best params\n# ================================\nbest = study.best_params\nmodel_type = best[\"model_type\"]\n\nprint(\"Training final model:\", model_type)\n\nif model_type == \"lightgbm\":\n    params = best.copy()\n    params[\"objective\"] = \"binary\"\n    params[\"metric\"] = \"auc\"\n    del params[\"model_type\"]\n\n    dtrain = lgb.Dataset(X, y)\n    final_model = lgb.train(params, dtrain, num_boost_round=2000)\n\nelif model_type == \"xgboost\":\n    params = best.copy()\n    params[\"objective\"] = \"binary:logistic\"\n    params[\"eval_metric\"] = \"auc\"\n    params[\"tree_method\"] = \"gpu_hist\"\n    del params[\"model_type\"]\n\n    final_model = xgb.train(params, xgb.DMatrix(X, y), num_boost_round=2000)\n\nelse:  # catboost\n    params = best.copy()\n    params[\"loss_function\"] = \"Logloss\"\n    params[\"eval_metric\"] = \"AUC\"\n    params[\"task_type\"] = \"GPU\"\n    del params[\"model_type\"]\n\n    final_model = CatBoostClassifier(**params)\n    final_model.fit(X, y)\n\n\n# ================================\n# 6. Predict test & save submission\n# ================================\nif model_type == \"lightgbm\":\n    preds = final_model.predict(test_X)\n\nelif model_type == \"xgboost\":\n    preds = final_model.predict(xgb.DMatrix(test_X))\n\nelse:\n    preds = final_model.predict_proba(test_X)[:, 1]\n\nsub = pd.DataFrame({\n    \"SK_ID_CURR\": test_ids,\n    \"TARGET\": preds\n})\n\nsub.to_csv(\"submission_optuna.csv\", index=False)\nprint(\"Saved submission_optuna.csv\")\n\njoblib.dump(final_model, \"best_optuna_model.pkl\")\nprint(\"Saved best_optuna_model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T15:17:48.900100Z","iopub.execute_input":"2025-11-21T15:17:48.900529Z","iopub.status.idle":"2025-11-21T16:22:51.779465Z","shell.execute_reply.started":"2025-11-21T15:17:48.900498Z","shell.execute_reply":"2025-11-21T16:22:51.778691Z"}},"outputs":[{"name":"stdout","text":"GPU test:\nFri Nov 21 15:17:52 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0             32W /  250W |     259MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-21 15:17:53,022] A new study created in memory with name: no-name-d739792c-836a-4cd8-bef1-902e4a9fab7c\n[I 2025-11-21 15:20:55,794] Trial 0 finished with value: 0.7675338709340829 and parameters: {'model_type': 'xgboost', 'eta': 0.12368271868631443, 'max_depth': 8, 'subsample': 0.7910402116677808, 'colsample_bytree': 0.5544278945624586}. Best is trial 0 with value: 0.7675338709340829.\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\nDefault metric period is 5 because AUC is/are not implemented for GPU\n[I 2025-11-21 15:22:17,588] Trial 1 finished with value: 0.7706824811809779 and parameters: {'model_type': 'catboost', 'lr': 0.2678978841127362, 'depth': 8}. Best is trial 1 with value: 0.7706824811809779.\n[I 2025-11-21 15:25:31,691] Trial 2 finished with value: 0.7528873668569054 and parameters: {'model_type': 'lightgbm', 'lr': 0.2985398272791956, 'num_leaves': 131, 'feature_fraction': 0.6456355064218796, 'bagging_fraction': 0.8045786179116066, 'bagging_freq': 3, 'min_data_in_leaf': 46}. Best is trial 1 with value: 0.7706824811809779.\n[I 2025-11-21 15:46:32,187] Trial 3 finished with value: 0.7792448155176982 and parameters: {'model_type': 'lightgbm', 'lr': 0.0196748929443546, 'num_leaves': 226, 'feature_fraction': 0.6531519961265604, 'bagging_fraction': 0.7796593941777206, 'bagging_freq': 1, 'min_data_in_leaf': 113}. Best is trial 3 with value: 0.7792448155176982.\n[I 2025-11-21 15:49:31,523] Trial 4 finished with value: 0.731485747365837 and parameters: {'model_type': 'xgboost', 'eta': 0.2505578189625315, 'max_depth': 10, 'subsample': 0.6473001028773179, 'colsample_bytree': 0.8842067528825904}. Best is trial 3 with value: 0.7792448155176982.\n[I 2025-11-21 15:53:24,725] Trial 5 finished with value: 0.773361935663652 and parameters: {'model_type': 'lightgbm', 'lr': 0.11876182332523418, 'num_leaves': 54, 'feature_fraction': 0.7916465049342389, 'bagging_fraction': 0.7091360686695486, 'bagging_freq': 1, 'min_data_in_leaf': 134}. Best is trial 3 with value: 0.7792448155176982.\n[I 2025-11-21 16:06:35,363] Trial 6 finished with value: 0.7768426428452326 and parameters: {'model_type': 'lightgbm', 'lr': 0.03610416783497944, 'num_leaves': 202, 'feature_fraction': 0.8605372049591036, 'bagging_fraction': 0.7594081663501955, 'bagging_freq': 6, 'min_data_in_leaf': 172}. Best is trial 3 with value: 0.7792448155176982.\n[I 2025-11-21 16:09:33,623] Trial 7 finished with value: 0.7629414336571447 and parameters: {'model_type': 'lightgbm', 'lr': 0.26233215788990955, 'num_leaves': 53, 'feature_fraction': 0.6113001365187057, 'bagging_fraction': 0.7622322091033819, 'bagging_freq': 7, 'min_data_in_leaf': 26}. Best is trial 3 with value: 0.7792448155176982.\n[I 2025-11-21 16:14:15,199] Trial 8 finished with value: 0.7559770981006336 and parameters: {'model_type': 'lightgbm', 'lr': 0.2513698689658768, 'num_leaves': 193, 'feature_fraction': 0.9568841326821929, 'bagging_fraction': 0.8799965325701473, 'bagging_freq': 3, 'min_data_in_leaf': 113}. Best is trial 3 with value: 0.7792448155176982.\n[I 2025-11-21 16:18:18,874] Trial 9 finished with value: 0.7792237735207632 and parameters: {'model_type': 'xgboost', 'eta': 0.039170107640165625, 'max_depth': 6, 'subsample': 0.5944220754358366, 'colsample_bytree': 0.8780912949225345}. Best is trial 3 with value: 0.7792448155176982.\n","output_type":"stream"},{"name":"stdout","text":"Best params: {'model_type': 'lightgbm', 'lr': 0.0196748929443546, 'num_leaves': 226, 'feature_fraction': 0.6531519961265604, 'bagging_fraction': 0.7796593941777206, 'bagging_freq': 1, 'min_data_in_leaf': 113}\nBest AUC: 0.7792448155176982\nTraining final model: lightgbm\nSaved submission_optuna.csv\nSaved best_optuna_model.pkl\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}